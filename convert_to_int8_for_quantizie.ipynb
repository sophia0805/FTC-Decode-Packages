{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f7d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 17:28:39.772474: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-09-05 17:28:39.772885: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-05 17:28:39.772958: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-05 17:28:39.773186: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-05 17:28:39.773472: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Load the .pb file\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f8eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 17:28:58.708590: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-09-05 17:28:58.708679: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-09-05 17:28:58.714411: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:28:58.781709: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-09-05 17:28:58.781742: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:28:58.987902: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2025-09-05 17:28:59.071189: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-09-05 17:28:59.987264: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:29:00.792899: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 2078973 microseconds.\n",
      "2025-09-05 17:29:01.744933: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()\n",
    "with open(\"final.tflite\", \"wb\") as f:\n",
    "   f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c865a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 17:40:18.059356: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-09-05 17:40:18.059469: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-09-05 17:40:18.059731: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:40:18.101422: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-09-05 17:40:18.101442: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:40:18.325348: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-09-05 17:40:19.583116: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2025-09-05 17:40:20.349967: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 2290256 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 quantized model saved as final_int8.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "saved_model_dir = \"models/mymodel/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\"\n",
    "quantized_model_path = \"final_int8.tflite\"\n",
    "\n",
    "# Load SavedModel\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# Optimization for full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Set input shape\n",
    "input_shape = [1, 320, 320, 3]  # adjust according to your model\n",
    "\n",
    "# Representative dataset for calibration\n",
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        # Generate random input in [0, 255] and type uint8\n",
    "        data = (np.random.rand(*input_shape) * 255).astype(np.uint8)\n",
    "        yield [data]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# Force full INT8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Input/output must be INT8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert and save\n",
    "tflite_quant_model = converter.convert()\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(f\"INT8 quantized model saved as {quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
